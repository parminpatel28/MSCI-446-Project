{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "The purpose of this notebook is to scrape basketball-reference.com. Using custom functions to scrape certain parts of the website, this code will go through every single game from the 2018 season to the 2024 season using PlayWright and download its HTML file using BeautifulSoup. It will then save each games HTML file under the directory '/data'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Logic\n",
    "\n",
    "- Create pointer variables to save .html files to a particular directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "STANDINGS_DIR = os.path.join(DATA_DIR, \"standings\")\n",
    "SCORES_DIR = os.path.join(DATA_DIR, \"scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HTML\n",
    "- Leverages Playwright to launch a browser instance and navigate to a particular URL passed in through the parameters. Based on a certain HTML selector passed in as well (basically a tag), it will select that part of the HTML file and return it. \n",
    "- Retries are implemented in the scenario where we face a PlaywrightTimeout error. We will retry 3 times, and pause the program for 3, 6, or 9 seconds depending on how many times we have faced the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_html(url, selector, sleep=3.01, retries=3):\n",
    "    html = None\n",
    "    for i in range(1, retries + 1):\n",
    "        # to avoid getting rate-limited, we employ time.sleep to wait between requests\n",
    "        time.sleep(sleep * i)\n",
    "\n",
    "        try:\n",
    "            async with async_playwright() as p:\n",
    "                # launch the browser instance with playwright\n",
    "                browser = await p.firefox.launch()\n",
    "\n",
    "                # create a new tab within the browser\n",
    "                page = await browser.new_page()\n",
    "\n",
    "                # navigate to the particular url using the new tab given \n",
    "                await page.goto(url)\n",
    "\n",
    "                print(await page.title())\n",
    "\n",
    "                html = await page.inner_html(selector)\n",
    "\n",
    "        except PlaywrightTimeout:\n",
    "            print(f\"Timeout error on {url}\")\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Season\n",
    "\n",
    "- Based on the structure of basketball-reference.com, each NBA season is seperated into months. To scrape each month in a season, we need to write a custom function to visit each month's HTML page because they reside at different URLs – we achieve this by finding each href. \n",
    "- We then go through each month, download its HTML file, and save it to a directory in the '/data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_season(season):\n",
    "    url = f\"https://www.basketball-reference.com/leagues/NBA_{season}_games.html\"\n",
    "    html = await get_html(url, \"#content .filter\")\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    links = soup.find_all(\"a\")\n",
    "    hrefs = [l['href'] for l in links] \n",
    "    standings_pages = [f\"https://basketball-reference.com{l}\" for l in hrefs]\n",
    "    \n",
    "    for url in standings_pages:\n",
    "        save_path = os.path.join(STANDINGS_DIR, url.split(\"/\")[-1])\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "\n",
    "        html = await get_html(url, \"#all_schedule\")\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Season Execution\n",
    "\n",
    "- Since we are interested in seasons from 2018 to 2025, we create a list of seasons and iterate through them to pass them into our scrape_season() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = list(range(2018, 2025))\n",
    "for season in seasons:\n",
    "    await scrape_season(season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standings Files\n",
    "\n",
    "- Each 'standing file' refers to a particular months schedule. In other words, it is an HTML file that contains an overview of each game played in a particular month. For example, \"NBA_2018_games-april.html\" will contain a generic overview of each game played in the 2018 NBA season in April specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standings_files = os.listdir(STANDINGS_DIR)\n",
    "standings_files = [s for s in standings_files if \"html\" in s]\n",
    "standings_files.sort(reverse=True)\n",
    "\n",
    "standings_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Game\n",
    "\n",
    "- Now that we have each months schedule for every season, we need to iterate through the games themselves now. We first open each 'standing file' and parse through each game to find its boxscore href. In other words, the link that leads to their boxscore page. We leverage the get_html() function with a specific selector to scrape each individual game in a season now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_game(standings_file):\n",
    "    with open(standings_file, 'r') as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    links = soup.find_all(\"a\")\n",
    "    hrefs = [l.get('href') for l in links]\n",
    "    box_scores = [f\"https://www.basketball-reference.com{l}\" for l in hrefs if l and \"boxscore\" in l and '.html' in l]\n",
    "\n",
    "    for url in box_scores:\n",
    "        save_path = os.path.join(SCORES_DIR, url.split(\"/\")[-1])\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "\n",
    "        html = await get_html(url, \"#content\")\n",
    "        if not html:\n",
    "            continue\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Game Execution \n",
    "\n",
    "- For every single month in a season, we call scrape_game() to get every game's HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in standings_files:\n",
    "    filepath = os.path.join(STANDINGS_DIR, f)\n",
    "\n",
    "    await scrape_game(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
